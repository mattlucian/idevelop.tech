{
  "title": "DevOps",
  "overview": "6+ years building automated CI/CD pipelines with GitHub Actions, CodeBuild, and CodeDeploy using multi-stage testing gates. Implemented comprehensive observability stack with DataDog and GrayLog including APMs, custom dashboards, and intelligent anomaly detection. Integrated alerting across PagerDuty and Slack for rapid incident response.",
  "badges": [
    {
      "label": "Experience",
      "value": "6+ Years",
      "icon": "â±ï¸"
    },
    {
      "label": "CI/CD",
      "value": "CI/CD Focus",
      "icon": "ðŸ”„"
    },
    {
      "label": "Monitoring",
      "value": "Anomaly Detection",
      "icon": "ðŸ“Š"
    },
    {
      "label": "Tools",
      "value": "DataDog, CodeBuild, GitHub",
      "icon": "ðŸ”§"
    }
  ],
  "categories": [
    {
      "name": "CI/CD & Build Systems",
      "subtitle": "Multi-stage pipelines with automated quality gates",
      "topics": [
        {
          "title": "AWS CodePipeline & CodeBuild",
          "subtitle": "Multi-stage automated pipelines",
          "skillTags": [
            "Source â†’ Build â†’ Test â†’ Deploy",
            "CodeBuild for Tests & Artifacts",
            "Multi-Environment Deployments",
            "Build Time Optimization",
            "Parallel Processing"
          ],
          "intro": "Built automated CI/CD pipelines using AWS CodePipeline and CodeBuild, implementing multi-stage deployments from GitHub to production with integrated testing and quality gates.",
          "sections": [
            {
              "heading": "Typical Pipeline Architecture",
              "content": "Standard pipeline flow: Source (GitHub webhook) â†’ Build (CodeBuild builds artifact, runs unit tests, passes to next stage) â†’ Test (CodeBuild pulls local resources like Flyway repo, test containers for S3/Redis, runs integration tests) â†’ Deploy (CodeDeploy to ECS/Elastic Beanstalk, or deploy library to S3). Variations existed based on application complexity - simpler apps had fewer stages, complex apps had additional quality gates."
            },
            {
              "heading": "CodeBuild Test Integration",
              "content": "Leveraged CodeBuild to run tests within build stages. Unit tests ran during Build stage for fast feedback. Integration tests requiring test containers ran in separate Test stage. Application complexity dictated whether tests ran in same buildspec or separate stages. Load tests and performance tests typically excluded from automated pipeline."
            },
            {
              "heading": "Build Time Optimization",
              "content": "Build times increased with codebase size, especially in Java. Combatted with: (1) Reduced dependency bloat wherever possible, (2) Separated unit vs integration tests - only ran tests not requiring full application context in build stage, (3) Increased build server performance and enabled parallel processing, (4) Implemented caching strategy for external dependencies (Maven local repo caching). Testing time typically bigger issue than compilation - solved by segregating test levels required at different pipeline gates."
            },
            {
              "heading": "Multi-Environment Strategy",
              "content": "Environment management varies by organization. Best practice: separate at least dev, staging, and production shared environments. If possible, allocate feature-branch or per-developer environments (local or ad-hoc via IaC). Each environment correlated to git branch. Gitflow typically: feature branch â†’ dev â†’ staging â†’ production. In some setups, dev never merged into staging (only feature branches); in others, dev merged to staging on cadenced schedule."
            }
          ]
        },
        {
          "title": "GitHub Actions & Integration",
          "subtitle": "PR automation and quality checks",
          "skillTags": [
            "Branch Protection Rules",
            "PR Content Validation",
            "GitHub-Hosted Runners",
            "AI Code Review Experimentation",
            "Complementing CodePipeline"
          ],
          "intro": "Used GitHub Actions to enforce branch protection, automate PR quality checks, and prevent direct commits to main branch. Experimented with AI-powered code review but found limited value at the time.",
          "sections": [
            {
              "heading": "Branch Protection Automation",
              "content": "GitHub Actions prevented staff from opening PRs directly against main branch. Staging â†’ main merges restricted to ensure proper testing flow. Actions enforced proper git workflow and prevented bypassing quality gates."
            },
            {
              "heading": "PR Validation Checks",
              "content": "Actions validated PR content: checking for description completeness, linked issue tickets, proper labeling, required approvers. Simple quality gates that ran before code review to ensure PRs had necessary context for reviewers."
            },
            {
              "heading": "AI Code Review Experimentation",
              "content": "Experimented with AI code review bots for PRs at one point. At that stage, AI wasn't helpful enough relative to cost - flagged false positives, missed real issues, provided generic feedback. Good opportunity to revisit with modern LLMs (2024+) which have significantly improved."
            },
            {
              "heading": "Complementing AWS CodePipeline",
              "content": "GitHub Actions handled pre-merge checks and branch protection. CodePipeline handled post-merge build, test, and deployment. Clean separation: GitHub Actions for code quality gates, AWS services for artifact creation and deployment automation."
            }
          ]
        },
        {
          "title": "Build Tool Optimization",
          "subtitle": "Maven, NPM, and dependency management",
          "skillTags": [
            "S3 Maven Repository",
            "AWS Auth Integration",
            "Build Caching",
            "Private Repository Strategy"
          ],
          "intro": "Deep expertise with Maven and NPM build tooling, including custom S3-based Maven repository to replace Artifactory, saving costs and improving security.",
          "sections": [
            {
              "heading": "S3 Maven Repository",
              "content": "Replaced Artifactory with S3-based Maven repository, saving costs and eliminating security burden of standalone server. Used aws-maven extension for S3 integration with AWS auth chain. Forked and enhanced aws-maven to support IAM role assumption for better auth support."
            },
            {
              "heading": "Build Configuration",
              "content": "Deep understanding of Maven extensions, plugins, and custom configurations. Built complex multi-module Maven projects with proper dependency management. NPM used for frontend builds - generally hands-off with Vue/Webpack/Vite, modern tooling works well out of box."
            },
            {
              "heading": "Private Repository Strategy",
              "content": "For private repos: S3 bucket as Maven repo with auth plugins avoids costs and maintenance burden of dedicated Artifactory/Nexus servers. Similar approach possible for NPM with S3 as private registry."
            }
          ]
        },
        {
          "title": "Deployment Strategies & Rollback",
          "subtitle": "Blue/green, rolling updates, and safe deployments",
          "skillTags": [
            "Blue/Green Deployments",
            "Rolling Updates",
            "Backwards-Compatible Changes",
            "Flyway Migration Strategy",
            "Multi-Stage Deployment Safety"
          ],
          "intro": "Implemented deployment strategies based on application state requirements. Emphasized deployment frequency capability (hourly) vs actual frequency (weekly) to enable rapid incident response. Comprehensive rollback philosophy centered on backwards compatibility.",
          "sections": [
            {
              "heading": "Blue/Green for Stateless APIs",
              "content": "Aimed to build stateless APIs, enabling blue/green deployments. New version deployed alongside old, traffic switched atomically once health checks pass. Instant rollback by switching traffic back. Clean separation between versions without in-place updates to running instances."
            },
            {
              "heading": "Rolling Updates for Stateful Services",
              "content": "For stateful APIs: performed rolling updates by shutting off traffic to instance, letting active responses complete, then taking instance down. Gradual rollout reduces blast radius of issues."
            },
            {
              "heading": "Deployment Philosophy: Weekly Deploy, Hourly Capable",
              "content": "Philosophy: Aim to deploy weekly, be capable of deploying hourly. Deployments should be routine enough to execute multiple times daily without issues. If deployments are cumbersome, creates major bottleneck during incidents. Critical bugs demand immediate deployment capability."
            },
            {
              "heading": "Comprehensive Rollback Philosophy",
              "content": "Every deployment comes with rollback plan. Big advantage of Flyway: backwards-compatible SQL/code changes. Data layer deploys first with changes compatible with current application code. Deploy stages: (1) Deploy backwards-compatible database schema, (2) Deploy new application code that works with old and new schema, (3) Verify stability, (4) Deploy cleanup database changes. Each stage independently rollbackable."
            }
          ]
        }
      ]
    },
    {
      "name": "Monitoring & Observability",
      "subtitle": "DataDog APM, logs, and intelligent alerting",
      "topics": [
        {
          "title": "DataDog Alerts & Metrics",
          "subtitle": "Application performance monitoring with anomaly detection",
          "skillTags": [
            "APM & Tracing",
            "Custom Metrics",
            "WatchDog Anomaly Detection",
            "Alert Thresholds",
            "Integration with PagerDuty/Slack"
          ],
          "intro": "Implemented comprehensive monitoring and alerting using DataDog APM, custom metrics, and WatchDog anomaly detection. Configured intelligent alerts that catch issues before customer impact while minimizing false positives.",
          "sections": [
            {
              "heading": "APM & Distributed Tracing",
              "content": "Integrated DataDog APM across microservices to trace requests through distributed systems. Identified performance bottlenecks, slow database queries, and external API latency issues. Used flame graphs and service maps to visualize request flows and optimize critical paths. APM provided visibility into application-level performance beyond infrastructure metrics."
            },
            {
              "heading": "Custom Metrics & Instrumentation",
              "content": "Implemented custom metrics for business-critical events: integration success/failure rates, job processing times, API response times by endpoint, cache hit rates. Used StatsD and DogStatsD for application-level metric emission. Created dashboards showing both technical and business metrics for holistic monitoring."
            },
            {
              "heading": "WatchDog Anomaly Detection",
              "content": "Leveraged DataDog WatchDog for intelligent anomaly detection using machine learning. WatchDog automatically detected unusual patterns in metrics without manual threshold configuration - caught issues like gradual memory leaks, traffic pattern changes, and performance degradations. Reduced alert fatigue by focusing on statistically significant anomalies rather than arbitrary thresholds."
            },
            {
              "heading": "Alert Configuration & Thresholds",
              "content": "Configured tiered alerting: Warning alerts for early indicators (CPU >70%, error rate >1%), Critical alerts for immediate action (CPU >90%, error rate >5%, service unavailable). Used composite monitors combining multiple signals for more reliable alerts. Tuned alert sensitivity over time to balance responsiveness with noise reduction."
            },
            {
              "heading": "Integration with PagerDuty & Slack",
              "content": "Integrated DataDog alerts with PagerDuty for on-call escalation and Slack for team visibility. Critical alerts paged on-call engineer immediately. Warning alerts posted to Slack channels for awareness. Configured alert routing based on service ownership and severity. This multi-channel approach ensured rapid response while keeping teams informed."
            }
          ]
        },
        {
          "title": "Log Sampling & Retention",
          "subtitle": "GrayLog and DataDog log management",
          "skillTags": [
            "GrayLog for Complete Retention",
            "DataDog for Sampled Logs",
            "Log Parsing & Structured Logging",
            "Cost vs Coverage Trade-offs",
            "Query & Search Capabilities"
          ],
          "intro": "Implemented dual logging strategy using GrayLog for complete log retention and DataDog for sampled logs with APM correlation. Balanced cost, retention requirements, and query capabilities through strategic log routing and sampling.",
          "sections": [
            {
              "heading": "GrayLog for Complete Retention",
              "content": "Used GrayLog as primary log aggregation platform for complete log retention across all services. GrayLog provided cost-effective storage for high-volume logs with flexible retention policies. Configured inputs from Docker containers, ECS tasks, and application loggers. Maintained 30-90 day retention for compliance and debugging without DataDog's per-GB ingestion costs."
            },
            {
              "heading": "DataDog for Sampled Logs & APM",
              "content": "Sent sampled logs to DataDog (typically 10-20% of total volume) to reduce costs while maintaining APM trace correlation. Sampled ERROR and WARN logs at 100%, INFO logs at lower rates. DataDog's log-to-trace correlation enabled debugging performance issues by linking logs to specific requests. This hybrid approach balanced DataDog's superior analysis capabilities with cost constraints."
            },
            {
              "heading": "Structured Logging & Parsing",
              "content": "Implemented structured JSON logging across all applications for consistent parsing. Included standard fields: timestamp, log level, service name, trace ID, message, context. Used GrayLog extractors and DataDog pipelines to parse and enrich logs. Structured logging enabled efficient queries, aggregations, and dashboards without regex parsing of unstructured text."
            },
            {
              "heading": "Cost vs Coverage Trade-offs",
              "content": "Made conscious trade-offs between log coverage and costs. GrayLog handled complete retention at lower cost but with basic query capabilities. DataDog provided superior search, correlation, and dashboards but at premium pricing. Routed logs strategically: all logs to GrayLog for retention, sampled critical logs to DataDog for active monitoring. This approach kept costs manageable while maintaining comprehensive log coverage."
            },
            {
              "heading": "Query & Search Capabilities",
              "content": "Used GrayLog for historical searches and pattern analysis across complete log dataset. Leveraged DataDog for real-time debugging and APM-correlated investigations. Both systems provided field-based queries and filtering. For cost-sensitive use cases, queried GrayLog first; for time-sensitive production issues, used DataDog's faster interface and trace correlation."
            }
          ]
        },
        {
          "title": "System Performance Dashboards",
          "subtitle": "Custom monitoring visualizations",
          "skillTags": [
            "Service-Level Dashboards",
            "Infrastructure Metrics",
            "Business Metrics",
            "Error Tracking & Analysis",
            "Real-Time Monitoring"
          ],
          "intro": "Built comprehensive custom dashboards combining infrastructure metrics, application performance, and business KPIs. Designed for both real-time monitoring and historical analysis to support operations and strategic decision-making.",
          "sections": [
            {
              "heading": "Service-Level Dashboards",
              "content": "Created dashboards for each critical service showing: request rate, latency percentiles (p50, p95, p99), error rates, and throughput. Included comparison views (current vs previous week) to identify trends. Service owners monitored their dashboards for performance regressions and capacity planning. Dashboards served as first stop during incident response."
            },
            {
              "heading": "Infrastructure Metrics",
              "content": "Built infrastructure overview dashboards showing: ECS task counts and resource utilization, RDS performance metrics (CPU, connections, IOPS), ElastiCache hit rates and memory usage, ALB request counts and target health. Configured automatic refresh for real-time visibility. Used these dashboards to identify scaling needs and optimization opportunities."
            },
            {
              "heading": "Business Metrics Integration",
              "content": "Incorporated business-level metrics alongside technical metrics: integration processing rates, job success/failure ratios, API usage by customer, revenue-impacting transactions. Custom metrics emitted from applications using StatsD. This unified view helped correlate technical issues with business impact and prioritize incident response."
            },
            {
              "heading": "Error Tracking & Root Cause Analysis",
              "content": "Created dedicated error dashboards showing: error rates by service and endpoint, error distributions by type/status code, top error messages with occurrence counts. Linked errors to log entries and APM traces for rapid root cause analysis. Used error trend analysis to identify emerging issues before they became critical. Dashboards tracked error reduction initiatives and quality improvements over time."
            },
            {
              "heading": "Real-Time Monitoring & Historical Analysis",
              "content": "Configured dashboards for dual purposes: real-time monitoring with 1-minute granularity for operations, historical views with 1-day or 1-week ranges for trend analysis and capacity planning. Used template variables to switch between environments (prod, staging, dev) and time ranges. Shared dashboard links in incident channels for team situational awareness during outages."
            }
          ]
        }
      ]
    },
    {
      "name": "Containerization",
      "subtitle": "Docker and ECS container management",
      "topics": [
        {
          "title": "Docker & ECS Container Management",
          "subtitle": "Container build, deployment, and orchestration",
          "skillTags": [
            "Docker Build & Optimization",
            "Amazon ECS Deployments",
            "ECR Image Management",
            "Container Best Practices"
          ],
          "intro": "Extensive experience with Docker containerization and Amazon ECS orchestration for production workloads. Built optimized container images and managed ECS deployments for scalable, reliable service delivery.",
          "sections": [
            {
              "heading": "Docker Build & Optimization",
              "content": "Built production Docker images with multi-stage builds, layer caching optimization, and minimal base images. Implemented security scanning and vulnerability management for container images. Optimized image sizes and build times through strategic layer ordering and .dockerignore usage."
            },
            {
              "heading": "Amazon ECS Deployments",
              "content": "Deployed and managed containerized applications on ECS with both EC2 and Fargate launch types. Configured task definitions, service auto-scaling, load balancer integration, and health checks. Implemented blue/green deployments and rolling updates for zero-downtime releases."
            },
            {
              "heading": "ECR Image Management",
              "content": "Managed container registries in Amazon ECR with image lifecycle policies, vulnerability scanning, and cross-account access. Automated image builds and pushes through CI/CD pipelines with proper tagging strategies (semantic versioning, git SHA tags)."
            },
            {
              "heading": "Container Best Practices",
              "content": "Followed container best practices: running as non-root user, minimal attack surface, immutable infrastructure, proper signal handling, health checks, resource limits, and logging to stdout/stderr for centralized log aggregation."
            }
          ]
        }
      ]
    },
    {
      "name": "Incident Response",
      "subtitle": "Alerting, on-call, and incident management",
      "topics": [
        {
          "title": "Incident Alerting Integrations",
          "subtitle": "PagerDuty, Slack, and Statuspage automation",
          "skillTags": [
            "PagerDuty On-Call Management",
            "Slack Alert Channels",
            "Statuspage Customer Communication",
            "Alert Routing & Escalation",
            "Multi-Channel Strategy"
          ],
          "intro": "Implemented comprehensive incident alerting across PagerDuty for on-call escalation, Slack for team awareness, and Statuspage for customer communication. Configured intelligent routing and escalation to ensure rapid response while minimizing alert fatigue.",
          "sections": [
            {
              "heading": "PagerDuty On-Call Management",
              "content": "Configured PagerDuty as primary on-call alerting platform with escalation policies and schedules. Critical alerts from DataDog, CloudWatch, and monitoring systems triggered immediate pages to on-call engineer. Implemented escalation paths: initial page to primary on-call, escalate to secondary after 15 minutes, escalate to engineering manager after 30 minutes. Used PagerDuty mobile app for acknowledgment and incident management from anywhere."
            },
            {
              "heading": "Slack Alert Channels",
              "content": "Created dedicated Slack channels for automated alerts and incident coordination. Warning-level alerts posted to team channels for visibility without paging. Critical alerts posted to incident response channel along with PagerDuty page. Configured Slack workflows for incident declaration, status updates, and post-mortem scheduling. Team members monitored channels for situational awareness even when not on-call."
            },
            {
              "heading": "Statuspage Customer Communication",
              "content": "Used Statuspage for transparent customer communication during incidents. Configured component status indicators for API, integrations, and critical services. Posted incident updates as issues progressed through investigation, identified, monitoring, and resolved states. Automated Statuspage updates from PagerDuty incidents for consistency. Subscribers received email/SMS notifications about service disruptions affecting their usage."
            },
            {
              "heading": "Alert Routing & Severity Tiers",
              "content": "Configured intelligent alert routing based on severity and service ownership: Critical (production down, data loss risk) â†’ PagerDuty + Slack incident channel + Statuspage, Warning (degraded performance, non-critical errors) â†’ Slack team channel only, Info (deployment notifications, scaling events) â†’ Slack notifications channel. This tiered approach balanced rapid critical response with reduced noise for lower-priority events."
            },
            {
              "heading": "Integration Architecture",
              "content": "All monitoring sources (DataDog, CloudWatch, application health checks) sent alerts to centralized routing layer. Routing logic determined appropriate destinations based on severity, time of day, and service criticality. PagerDuty integrations with DataDog and AWS enabled automatic incident creation with full context (metrics, logs, traces). Slack integrations provided rich message formatting with graphs, runbook links, and quick actions."
            }
          ]
        },
        {
          "title": "AWS Runbooks",
          "subtitle": "Documented incident response procedures",
          "skillTags": [
            "Runbook Creation",
            "Common Incident Scenarios",
            "Step-by-Step Remediation",
            "Runbook Maintenance",
            "Team Training"
          ],
          "intro": "Created comprehensive runbooks documenting incident response procedures for common failure scenarios. Enabled rapid response by providing step-by-step remediation guides accessible during high-pressure incidents. Reduced mean time to resolution and enabled junior engineers to handle incidents independently.",
          "sections": [
            {
              "heading": "Runbook Structure & Content",
              "content": "Structured runbooks with: (1) Incident detection - symptoms and monitoring alerts indicating the issue, (2) Initial assessment - diagnostic steps to confirm root cause, (3) Remediation steps - specific commands and procedures to resolve, (4) Verification - how to confirm resolution, (5) Communication - when and how to update stakeholders. Included AWS CLI commands, SQL queries, and links to relevant dashboards. Made runbooks actionable and concise for use during incidents."
            },
            {
              "heading": "Common Incident Scenarios",
              "content": "Documented runbooks for recurring incidents: Database performance degradation (check slow query log, restart read replicas, scale instance), ECS task failures (check CloudWatch logs, verify IAM roles, restart tasks), Integration job failures (check error logs, replay failed jobs, verify external API connectivity), Cache cluster failures (failover to replica, provision new cluster, restore from backup). Covered 80% of incidents with standard runbooks."
            },
            {
              "heading": "Runbook Accessibility",
              "content": "Stored runbooks in Confluence with clear organization and search. Linked runbooks from PagerDuty alerts and Slack notifications for immediate access during incidents. Created quick reference guide with links to all runbooks for on-call engineers. Ensured runbooks accessible from mobile devices for engineers responding remotely. Discoverability crucial - runbooks useless if not found when needed."
            },
            {
              "heading": "Runbook Maintenance & Updates",
              "content": "Updated runbooks after each incident during post-mortem if steps changed or new resolution discovered. Tested runbooks quarterly during incident response drills. Assigned runbook ownership to teams responsible for systems. Flagged stale runbooks needing review. Living documents - outdated runbooks worse than no runbooks as they wasted time and created confusion."
            },
            {
              "heading": "Training & Incident Drills",
              "content": "Conducted incident response training using runbooks for new engineers joining on-call rotation. Ran incident simulations (GameDays) where team practiced using runbooks for hypothetical scenarios. Validated runbook accuracy and completeness through drills. Training built confidence and muscle memory for real incidents. Engineers familiar with runbooks responded faster and more effectively."
            }
          ]
        },
        {
          "title": "Post-Mortem Analysis",
          "subtitle": "Learning from incidents and preventing recurrence",
          "skillTags": [
            "Blameless Post-Mortems",
            "Root Cause Analysis",
            "Action Items",
            "Knowledge Sharing",
            "Incident Trends"
          ],
          "intro": "Conducted blameless post-mortems after significant incidents to understand root causes and prevent recurrence. Fostered learning culture focused on system improvements rather than individual blame. Tracked incident trends to identify systemic issues requiring architectural changes.",
          "sections": [
            {
              "heading": "Blameless Post-Mortem Culture",
              "content": "Established blameless post-mortem culture: incidents viewed as system failures, not individual mistakes. Focus on understanding what happened and improving systems/processes. Engineers comfortable participating honestly without fear of punishment. Named \"learning reviews\" rather than \"post-mortems\" to emphasize improvement mindset. Culture shift required executive buy-in and consistent modeling of blameless approach."
            },
            {
              "heading": "Post-Mortem Structure",
              "content": "Standard post-mortem template: (1) Timeline - chronological events from detection to resolution, (2) Impact - affected customers, duration, revenue impact, (3) Root cause - underlying system or process failure, (4) Contributing factors - conditions enabling the incident, (5) What went well - effective response actions, (6) What needs improvement - gaps revealed by incident, (7) Action items - specific tasks to prevent recurrence. Led post-mortem meetings within 48 hours of major incidents while details fresh."
            },
            {
              "heading": "Root Cause Analysis Techniques",
              "content": "Used \"5 Whys\" technique drilling into root causes: asked \"why\" repeatedly until reaching systemic issue rather than proximate cause. Example: Service down (why?) â†’ Task crashed (why?) â†’ Out of memory (why?) â†’ Memory leak in new code (why?) â†’ Insufficient code review/testing (systemic issue). Root cause often process/tooling gap, not code bug. Action items addressed root cause to prevent similar incidents."
            },
            {
              "heading": "Action Item Tracking",
              "content": "Created JIRA tickets for all action items with owners and due dates. Categorized actions: immediate (hotfix deployed within 24 hours), short-term (improvements within 1-2 sprints), long-term (architectural changes requiring quarters). Reviewed action item completion in weekly engineering meetings. Held teams accountable to completing actions - post-mortems worthless without follow-through on improvements."
            },
            {
              "heading": "Knowledge Sharing & Trends",
              "content": "Shared post-mortems broadly with engineering organization for learning. Published sanitized versions to company for transparency. Analyzed incident trends quarterly: increasing database incidents â†’ prioritize database optimization, recurring deployment issues â†’ improve deployment automation. Trends informed technical strategy and investment priorities. Learning from incidents made systems more resilient over time."
            }
          ]
        }
      ]
    },
    {
      "name": "Infrastructure as Code",
      "subtitle": "IaC prototyping and governance",
      "topics": [
        {
          "title": "IaC Tools & Prototyping",
          "subtitle": "CloudFormation, Terraform, and serverless IaC",
          "skillTags": [
            "CloudFormation Prototyping",
            "Terraform Experimentation",
            "SST.dev for Serverless",
            "IAM Governance"
          ],
          "intro": "Experience with Infrastructure as Code tools for prototyping and automation. Primarily used CloudFormation and experimented with Terraform and modern serverless frameworks.",
          "sections": [
            {
              "heading": "CloudFormation Prototyping",
              "content": "Used CloudFormation for AWS infrastructure provisioning and prototyping. Familiar with template structure, stack management, and AWS native IaC patterns. Leveraged CloudFormation for consistent, repeatable infrastructure deployments."
            },
            {
              "heading": "Terraform Experimentation",
              "content": "Experimented with Terraform for multi-cloud IaC capabilities. Understanding of HCL syntax, state management, and module system. Evaluated Terraform vs CloudFormation for various use cases."
            },
            {
              "heading": "SST.dev for Serverless Projects",
              "content": "Explored SST.dev for serverless infrastructure deployment. Modern approach to serverless IaC with TypeScript support and local development workflows. Particularly useful for Lambda-based applications."
            },
            {
              "heading": "IAM Governance & Access Controls",
              "content": "Implemented IAM governance policies and access controls through IaC. Defined least-privilege IAM roles, service-specific policies, and cross-account access patterns. Automated IAM configuration for consistency and security."
            }
          ]
        }
      ]
    }
  ]
}
